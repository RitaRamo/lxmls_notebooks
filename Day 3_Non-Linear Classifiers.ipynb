{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Classifiers\n",
    "\n",
    "Today’s class will introduce modern neural network models, commonly known as deep learning models. We will learn the concept of computation graph, a general way of describing complex functions as composition of simpler functions. We will also learn about Backpropagation, a generic solution for gradient-descent based optimization in computation graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.1 </b>\n",
    "<br>\n",
    "To ease-up the upcoming implementation exercise, examine and comment the following implementation of a log-linear model and its gradient update rule. Start by loading Amazon sentiment corpus used in day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lxmls.readers.sentiment_reader as srs \n",
    "from lxmls.deep_learning.utils import AmazonData\n",
    "corpus=srs.SentimentCorpus(\"books\")\n",
    "data = AmazonData(corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print corpus.train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print corpus.train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances to test 1600\n",
      "Number of instances that belong to class 1:  800\n",
      "Number of instances that belong to class 0:  800\n"
     ]
    }
   ],
   "source": [
    "print \"Number of instances to test\", len(corpus.train_y)\n",
    "print \"Number of instances that belong to class 1: \",(corpus.train_y==1).sum()\n",
    "print \"Number of instances that belong to class 0: \",(corpus.train_y==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the following numpy implementation of a log-linear model with the derivations seen in the previous sections. Introduce comments on the blocks marked with # relating them to the corresponding algorithm steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.utils import Model, glorot_weight_init, index2onehot \n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "class NumpyLogLinear(Model):\n",
    "    def __init__(self, **config):\n",
    "        # Initialize parameters\n",
    "        weight_shape = (config['input_size'], config['num_classes'])\n",
    "        # after Xavier Glorot et al\n",
    "        self.weight = glorot_weight_init(weight_shape, 'softmax')\n",
    "        self.bias = np.zeros((1, config['num_classes']))\n",
    "        self.learning_rate = config['learning_rate']\n",
    "\n",
    "    def log_forward(self, input=None):\n",
    "        \"\"\"Forward pass of the computation graph\"\"\"\n",
    "        \n",
    "        #weighted sums of the node input plus a bias.  Sum(w*x)+b\n",
    "        z = np.dot(input, self.weight.T) + self.bias \n",
    "        \n",
    "        # Softmax implemented in log domain\n",
    "        log_tilde_z = z - logsumexp(z, axis=1)[:, None]\n",
    "        \n",
    "        return log_tilde_z\n",
    "    \n",
    "    def predict(self, input=None):\n",
    "        \"\"\"Prediction: most probable class index\"\"\"\n",
    "        return np.argmax(np.exp(self.log_forward(input)), axis=1)\n",
    "\n",
    "    def update(self, input=None, output=None): \n",
    "        \"\"\"Stochastic Gradient Descent update\"\"\"\n",
    "        \n",
    "        #compute class probabilities\n",
    "        class_probabilities = np.exp(self.log_forward(input))\n",
    "        batch_size, num_classes = class_probabilities.shape\n",
    "        \n",
    "        #Error derivative  of the cost function\n",
    "        I = index2onehot(output, num_classes)\n",
    "        error = (class_probabilities - I) / batch_size\n",
    "        \n",
    "        #Gradient with respect to the weights \n",
    "        gradient_weight = np.zeros(self.weight.shape) \n",
    "        for l in range(batch_size):\n",
    "            gradient_weight += np.outer(error[l, :], input[l, :])\n",
    "            \n",
    "        #Gradient with respect to the bias \n",
    "        gradient_bias = np.sum(error, axis=0, keepdims=True)\n",
    "        \n",
    "        #update our parameters estimates with gradient descent (W ← W − η∇WF; b ← b − η∇bF,)\n",
    "        self.weight = self.weight - self.learning_rate * gradient_weight\n",
    "        self.bias = self.bias - self.learning_rate * gradient_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instantiate model and data classes. Check the initial accuracy of the model. This should be close to 50% since we are on a binary prediction task and the model is not trained yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy 54.25 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RitaRamos/anaconda/envs/mypy26/lib/python2.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `logsumexp` is deprecated!\n",
      "Importing `logsumexp` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.logsumexp` instead.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = NumpyLogLinear(\n",
    "    input_size=corpus.nr_features,\n",
    "    num_classes=2,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "# Define number of epochs and batch size\n",
    "num_epochs = 10\n",
    "batch_size = 30\n",
    "# Instantiate data iterators\n",
    "train_batches = data.batches('train', batch_size=batch_size)\n",
    "test_set = data.batches('test', batch_size=None)[0]\n",
    "# Check initial accuracy\n",
    "hat_y = model.predict(input=test_set['input'])\n",
    "accuracy = 100*np.mean(hat_y == test_set['output']) \n",
    "print(\"Initial accuracy %2.2f %%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the model with simple batch stochastic gradient descent. Be sure to understand each of the steps involved, including the code running inside of the model class. We will be wokring on a more complex version of the model in the upcoming exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RitaRamos/anaconda/envs/mypy26/lib/python2.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `logsumexp` is deprecated!\n",
      "Importing `logsumexp` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.logsumexp` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: accuracy 70.50 %\n",
      "Epoch 2: accuracy 73.75 %\n",
      "Epoch 3: accuracy 76.25 %\n",
      "Epoch 4: accuracy 77.75 %\n",
      "Epoch 5: accuracy 78.75 %\n",
      "Epoch 6: accuracy 79.25 %\n",
      "Epoch 7: accuracy 79.25 %\n",
      "Epoch 8: accuracy 80.25 %\n",
      "Epoch 9: accuracy 80.50 %\n",
      "Epoch 10: accuracy 80.50 %\n"
     ]
    }
   ],
   "source": [
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Batch loop\n",
    "    for batch in train_batches:\n",
    "        model.update(input=batch['input'], output=batch['output'])\n",
    "\n",
    "    # Prediction for this epoch\n",
    "    hat_y = model.predict(input=test_set['input'])\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = 100*np.mean(hat_y == test_set['output'])\n",
    "    print(\"Epoch %d: accuracy %2.2f %%\" % (epoch+1, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.2  </b>\n",
    "<br>Instantiate the feed-forward model class and optimization parameters. This models follows the architecture described in Algorithm 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "geometry = [corpus.nr_features, 20, 2]\n",
    "activation_functions = ['sigmoid', 'softmax']\n",
    "# Optimization\n",
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 30\n",
    "# Instantiate model\n",
    "from lxmls.deep_learning.numpy_models.mlp import NumpyMLP \n",
    "\n",
    "model = NumpyMLP(\n",
    "    geometry=geometry,\n",
    "    activation_functions=activation_functions,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.mlp import get_mlp_parameter_handlers, get_mlp_loss_range\n",
    "# Get functions to get and set values of a particular weight of the model\n",
    "get_parameter, set_parameter = get_mlp_parameter_handlers(layer_index=1,is_bias=False, row=0, column=0)\n",
    "# Get batch of data\n",
    "batch = data.batches('train', batch_size=batch_size)[0]\n",
    "# Get loss and weight value\n",
    "current_loss = model.cross_entropy_loss(batch['input'], batch['output'])\n",
    "current_weight = get_parameter(model.parameters)\n",
    "# Get range of values of the weight and loss around current parameters values\n",
    "weight_range, loss_range = get_mlp_loss_range(model, get_parameter, set_parameter, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once you have implemented at least the gradient of the last layer. You can start checking if the values match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = model.backpropagation(batch['input'], batch['output'])\n",
    "current_gradient = get_parameter(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now you can plot the values of the loss around a given parameters value versus the gradient. If you have implemented this correctly the gradient should be tangent to the loss at the current weight value, see Figure 3.5. Once you have completed the exercise, you should be able to plot also the gradients of the other layers. Take into account that the gradients for the first layer will only be non zero for the indices of words present in the batch. You can locate this using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   59,   111,   125,   258,   302,   361,   514,   737,   743,\n",
       "          814,   825,   872,   922,   977,  1094,  1280,  1286,  1296,\n",
       "         1314,  1348,  1604,  1731,  1830,  1947,  2057,  2120,  2173,\n",
       "         2235,  2245,  2261,  2331,  2347,  2492,  2555,  2608,  2638,\n",
       "         2755,  2882,  3069,  3091,  3157,  3291,  3322,  3342,  3558,\n",
       "         3769,  4203,  4478,  4509,  4718,  5033,  5136,  5244,  5305,\n",
       "         5444,  5557,  5909,  6115,  6152,  6268,  6303,  6431,  6702,\n",
       "         6720,  6735,  6810,  6926,  6961,  6986,  7168,  7187,  7364,\n",
       "         7414,  7584,  7803,  7882,  8007,  8055,  8258,  8330,  8421,\n",
       "         8552,  8925,  9037,  9048,  9383,  9599,  9732,  9778,  9809,\n",
       "         9967, 10063, 10134, 10327, 10420, 10578, 10652, 10714, 10846,\n",
       "        10964, 11003, 11105, 11432, 11530, 11719, 11720, 11732, 11791,\n",
       "        11817, 11902, 11922, 11970, 11999, 12054, 12064, 12073, 12090,\n",
       "        12116, 12139, 12287, 12397, 12544, 12554, 12558, 12772, 13299,\n",
       "        13455, 13578, 13686, 13756, 13787, 13944]),)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input'][0].nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copy the following code for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a2809d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot empirical\n",
    "plt.plot(weight_range, loss_range)\n",
    "plt.plot(current_weight, current_loss, 'xr')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('weight value')\n",
    "# Plot real\n",
    "h = plt.plot(\n",
    "    weight_range,\n",
    "    current_gradient*(weight_range - current_weight) + current_loss,'r--'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After you have ensured that your Backpropagation algorithm is correct, you can train a model with the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: accuracy 64.25 %\n",
      "Epoch 2: accuracy 68.75 %\n",
      "Epoch 3: accuracy 73.75 %\n",
      "Epoch 4: accuracy 74.50 %\n",
      "Epoch 5: accuracy 76.50 %\n",
      "Epoch 6: accuracy 77.25 %\n",
      "Epoch 7: accuracy 79.75 %\n",
      "Epoch 8: accuracy 80.00 %\n",
      "Epoch 9: accuracy 81.00 %\n",
      "Epoch 10: accuracy 81.00 %\n"
     ]
    }
   ],
   "source": [
    "# Get batch iterators for train and test\n",
    "train_batches = data.batches('train', batch_size=batch_size)\n",
    "test_set = data.batches('test', batch_size=None)[0]\n",
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "# Batch loop\n",
    "    for batch in train_batches:\n",
    "        model.update(input=batch['input'], output=batch['output'])\n",
    "        # Prediction for this epoch\n",
    "    hat_y = model.predict(input=test_set['input'])\n",
    "    # Evaluation\n",
    "    accuracy = 100*np.mean(hat_y == test_set['output'])\n",
    "    print(\"Epoch %d: accuracy %2.2f %%\" % (epoch+1, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deriving gradients and GPU code with Pytorch\n",
    "<b> Exercise3.3 </b>\n",
    "<br> \n",
    "In order to learn the differences between a numpy and a Pytorch implementation, explore the reimplementation of Ex. 3.1 in Pytorch. Compare the content of each of the functions, in particular the forward() and update methods(). The comments indicated as IMPORTANT will highlight common sources of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class PytorchLogLinear(Model):\n",
    "    def __init__(self, **config):\n",
    "        # Initialize parameters\n",
    "        weight_shape = (config['input_size'], config['num_classes'])\n",
    "        \n",
    "        # after Xavier Glorot et al\n",
    "        self.weight = glorot_weight_init(weight_shape, 'softmax')\n",
    "        self.bias = np.zeros((1, config['num_classes']))\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        \n",
    "        # IMPORTANT: Cast to pytorch format\n",
    "        self.weight = Variable(torch.from_numpy(self.weight).float(), requires_grad=True)\n",
    "        self.bias = Variable(torch.from_numpy(self.bias).float(), requires_grad=True)\n",
    "        \n",
    "        # Instantiate softmax and negative logkelihood in log domain\n",
    "        self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.loss = torch.nn.NLLLoss()\n",
    "        \n",
    "    def _log_forward(self, input=None):\n",
    "        \"\"\"Forward pass of the computation graph in logarithm domain (pytorch)\"\"\"\n",
    "        # IMPORTANT: Cast to pytorch format\n",
    "        input = Variable(torch.from_numpy(input).float(), requires_grad=False) \n",
    "        \n",
    "        # Linear transformation\n",
    "        z = torch.matmul(input, torch.t(self.weight)) + self.bias \n",
    "        \n",
    "        # Softmax implemented in log domain\n",
    "        log_tilde_z = self.logsoftmax(z)\n",
    "        \n",
    "        # NOTE that this is a pytorch class!\n",
    "        return log_tilde_z\n",
    "    \n",
    "    def predict(self, input=None):\n",
    "        \"\"\"Most probably class index\"\"\"\n",
    "        log_forward = self._log_forward(input).data.numpy()\n",
    "        return np.argmax(np.exp(log_forward), axis=1)\n",
    "    \n",
    "    def update(self, input=None, output=None): \n",
    "        \"\"\"Stochastic Gradient Descent update\"\"\"    \n",
    "        true_class = Variable(torch.from_numpy(output).long(), requires_grad=False)\n",
    "\n",
    "        # Compute negative log-likelihood loss\n",
    "        loss = torch.nn.NLLLoss()(self._log_forward(input), true_class)\n",
    "        # Use autograd to compute the backward pass.\n",
    "        loss.backward()\n",
    "\n",
    "        # SGD update\n",
    "        self.weight.data -= self.learning_rate * self.weight.grad.data\n",
    "        self.bias.data -= self.learning_rate * self.bias.grad.data\n",
    "\n",
    "        # Zero gradients\n",
    "        self.weight.grad.data.zero_()\n",
    "        self.bias.grad.data.zero_()\n",
    "\n",
    "        return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once you understand the model you can instantiate it and run it using the standard training loop we have used on previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = PytorchLogLinear(\n",
    "    input_size=corpus.nr_features,\n",
    "    num_classes=2,\n",
    "    learning_rate=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: accuracy 70.50 %\n",
      "Epoch 2: accuracy 73.75 %\n",
      "Epoch 3: accuracy 76.25 %\n",
      "Epoch 4: accuracy 77.75 %\n",
      "Epoch 5: accuracy 78.75 %\n",
      "Epoch 6: accuracy 79.25 %\n",
      "Epoch 7: accuracy 79.25 %\n",
      "Epoch 8: accuracy 80.25 %\n",
      "Epoch 9: accuracy 80.50 %\n",
      "Epoch 10: accuracy 80.50 %\n"
     ]
    }
   ],
   "source": [
    "# Get batch iterators for train and test\n",
    "train_batches = data.batches('train', batch_size=batch_size)\n",
    "test_set = data.batches('test', batch_size=None)[0]\n",
    "\n",
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Batch loop\n",
    "    for batch in train_batches:\n",
    "        model.update(input=batch['input'], output=batch['output'])\n",
    "        \n",
    "    # Prediction for this epoch\n",
    "    hat_y = model.predict(input=test_set['input'])\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = 100*np.mean(hat_y == test_set['output'])\n",
    "    print(\"Epoch %d: accuracy %2.2f %%\" % (epoch+1, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.4 </b>\n",
    "<br> As the final exercise today implement the log forward() method in lxmls/deep learning/pytorch models/mlp.py. Use the previous exercise as reference. After you have completed this you can run both systems for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
