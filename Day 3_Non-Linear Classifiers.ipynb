{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Classifiers\n",
    "\n",
    "Today’s class will introduce modern neural network models, commonly known as deep learning models. We will learn the concept of computation graph, a general way of describing complex functions as composition of simpler functions. We will also learn about Backpropagation, a generic solution for gradient-descent based optimization in computation graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.1 </b>\n",
    "<br>\n",
    "To ease-up the upcoming implementation exercise, examine and comment the following implementation of a log-linear model and its gradient update rule. Start by loading Amazon sentiment corpus used in day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lxmls.readers.sentiment_reader as srs \n",
    "from lxmls.deep_learning.utils import AmazonData\n",
    "corpus=srs.SentimentCorpus(\"books\")\n",
    "data = AmazonData(corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print corpus.train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print corpus.train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances to test 1600\n",
      "Number of instances that belong to class 1:  800\n",
      "Number of instances that belong to class 0:  800\n"
     ]
    }
   ],
   "source": [
    "print \"Number of instances to test\", len(corpus.train_y)\n",
    "print \"Number of instances that belong to class 1: \",(corpus.train_y==1).sum()\n",
    "print \"Number of instances that belong to class 0: \",(corpus.train_y==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the following numpy implementation of a log-linear model with the derivations seen in the previous sections. Introduce comments on the blocks marked with # relating them to the corresponding algorithm steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.utils import Model, glorot_weight_init, index2onehot \n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "class NumpyLogLinear(Model):\n",
    "    def __init__(self, **config):\n",
    "        # Initialize parameters\n",
    "        weight_shape = (config['input_size'], config['num_classes'])\n",
    "        # after Xavier Glorot et al\n",
    "        self.weight = glorot_weight_init(weight_shape, 'softmax')\n",
    "        self.bias = np.zeros((1, config['num_classes']))\n",
    "        self.learning_rate = config['learning_rate']\n",
    "\n",
    "    def log_forward(self, input=None):\n",
    "        \"\"\"Forward pass of the computation graph\"\"\"\n",
    "        \n",
    "        #weighted sums of the node input plus a bias.  Sum(w*x)+b\n",
    "        z = np.dot(input, self.weight.T) + self.bias \n",
    "        \n",
    "        # Softmax implemented in log domain\n",
    "        log_tilde_z = z - logsumexp(z, axis=1)[:, None]\n",
    "        \n",
    "        return log_tilde_z\n",
    "    \n",
    "    def predict(self, input=None):\n",
    "        \"\"\"Prediction: most probable class index\"\"\"\n",
    "        return np.argmax(np.exp(self.log_forward(input)), axis=1)\n",
    "\n",
    "    def update(self, input=None, output=None): \n",
    "        \"\"\"Stochastic Gradient Descent update\"\"\"\n",
    "        \n",
    "        #compute class probabilities\n",
    "        class_probabilities = np.exp(self.log_forward(input))\n",
    "        batch_size, num_classes = class_probabilities.shape\n",
    "        \n",
    "        #Error derivative \n",
    "        I = index2onehot(output, num_classes)\n",
    "        error = (class_probabilities - I) / batch_size\n",
    "        \n",
    "        #Gradient of the cost ∇F with respect to the weights \n",
    "        gradient_weight = np.zeros(self.weight.shape) \n",
    "        for l in range(batch_size):\n",
    "            gradient_weight += np.outer(error[l, :], input[l, :])\n",
    "            \n",
    "        #Gradient of the cost ∇F with respect to the bias \n",
    "        gradient_bias = np.sum(error, axis=0, keepdims=True)\n",
    "        \n",
    "        #update our parameters estimates with gradient descent (W ← W − η∇WF; b ← b − η∇bF,)\n",
    "        self.weight = self.weight - self.learning_rate * gradient_weight\n",
    "        self.bias = self.bias - self.learning_rate * gradient_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instantiate model and data classes. Check the initial accuracy of the model. This should be close to 50% since we are on a binary prediction task and the model is not trained yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy 54.25 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RitaRamos/anaconda/envs/mypy26/lib/python2.7/site-packages/ipykernel/__main__.py:21: DeprecationWarning: `logsumexp` is deprecated!\n",
      "Importing `logsumexp` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.logsumexp` instead.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = NumpyLogLinear(\n",
    "    input_size=corpus.nr_features,\n",
    "    num_classes=2,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "# Define number of epochs and batch size\n",
    "num_epochs = 10\n",
    "batch_size = 30\n",
    "# Instantiate data iterators\n",
    "train_batches = data.batches('train', batch_size=batch_size)\n",
    "test_set = data.batches('test', batch_size=None)[0]\n",
    "# Check initial accuracy\n",
    "hat_y = model.predict(input=test_set['input'])\n",
    "accuracy = 100*np.mean(hat_y == test_set['output']) \n",
    "print(\"Initial accuracy %2.2f %%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train the model with simple batch stochastic gradient descent. Be sure to understand each of the steps involved, including the code running inside of the model class. We will be wokring on a more complex version of the model in the upcoming exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RitaRamos/anaconda/envs/mypy26/lib/python2.7/site-packages/ipykernel/__main__.py:21: DeprecationWarning: `logsumexp` is deprecated!\n",
      "Importing `logsumexp` from scipy.misc is deprecated in scipy 1.0.0. Use `scipy.special.logsumexp` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: accuracy 84.75 %\n"
     ]
    }
   ],
   "source": [
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Batch loop\n",
    "    for batch in train_batches:\n",
    "        model.update(input=batch['input'], output=batch['output'])\n",
    "\n",
    "    # Prediction for this epoch\n",
    "    hat_y = model.predict(input=test_set['input'])\n",
    "\n",
    "# Evaluation\n",
    "accuracy = 100*np.mean(hat_y == test_set['output'])\n",
    "print(\"Epoch %d: accuracy %2.2f %%\" % (epoch+1, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.2  </b>\n",
    "<br>Instantiate the feed-forward model class and optimization parameters. This models follows the architecture described in Algorithm 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "geometry = [corpus.nr_features, 20, 2]\n",
    "activation_functions = ['sigmoid', 'softmax']\n",
    "# Optimization\n",
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 30\n",
    "# Instantiate model\n",
    "from lxmls.deep_learning.numpy_models.mlp import NumpyMLP \n",
    "\n",
    "model = NumpyMLP(\n",
    "    geometry=geometry,\n",
    "    activation_functions=activation_functions,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mypy26]",
   "language": "python",
   "name": "conda-env-mypy26-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
