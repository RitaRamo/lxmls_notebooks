{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Perform sentiment analysis on a corpus of book reviews from Amazon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 1.1 - Generative Classifiers: NaÄ±ve Bayes </b> \n",
    "<br>\n",
    "In this exercise we will use the Amazon sentiment analysis data (Blitzer et al., 2007), where the goal is to classify text documents as expressing a positive or negative sentiment (i.e., a classification problem with two classes). We are going to focus on book reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.sentiment_reader as srs\n",
    "scr = srs.SentimentCorpus(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My instances to train have the shape (1600, 13989) and are the following:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " Their targets/outputs belongo to the classes [0 1] and are the following:\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      " My instances to test have the shape (400, 13989)\n",
      "\n",
      "So, each instance (row) represents a doc, being the doc represented as a bag-of-words  (each collum represents the freq of that word in that doc). Each document can either be classified as positive or a negative sentiment (1, ou 0, respectively).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "print \"My instances to train have the shape\", np.shape(scr.train_X), \"and are the following:\\n\", scr.train_X\n",
    "print \"\\n Their targets/outputs belongo to the classes\", np.unique(scr.train_y),\"and are the following:\\n\", scr.train_y \n",
    "\n",
    "print \"\\n My instances to test have the shape\", np.shape(scr.test_X)\n",
    "\n",
    "print '\\nSo, each instance (row) represents a doc, being the doc represented as a bag-of-words  (each collum represents the freq of that word in that doc). Each document can either be classified as positive or a negative sentiment (1, ou 0, respectively).'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " < 1. Implement the Naive Bayes algorithm. Open the file multinomial naive bayes.py, which is inside the classifiers folder. In the MultinomialNaiveBayes class you will find the train method. We have already placed some code in that file to help you get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This is the code that I implemented in the train method:\n",
    "''' what was given, where x and y where scr.train_X and trainY:'''\n",
    "x=scr.train_X\n",
    "y=scr.train_y\n",
    "# n_docs = no. of documents\n",
    "# n_words = no. of unique words\n",
    "n_docs, n_words = x.shape\n",
    "# classes = a list of possible classes\n",
    "classes = np.unique(y)\n",
    "# n_classes = no. of classes\n",
    "n_classes = np.unique(y).shape[0]\n",
    "\n",
    "# initialization of the prior and likelihood variables\n",
    "prior = np.zeros(n_classes)\n",
    "likelihood = np.zeros((n_words, n_classes))\n",
    "\n",
    "'''my solution'''\n",
    "\n",
    "for i in range(n_classes):\n",
    "    docs_of_class=x[np.where(y==classes[i])[0]] \n",
    "\n",
    "    prior[i]=len(docs_of_class)/n_docs\n",
    "\n",
    "    freq_of_each_word=docs_of_class.sum(0)  #[freq_w1, freq_w2, etc]\n",
    "    likelihood[:,i]=freq_of_each_word/ freq_of_each_word.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 2. After implementing, run NaiveBayes with the multinomial model on the Amazon dataset(sentiment classification) and report results both for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lxmls/classifiers/multinomial_naive_bayes.py:65: RuntimeWarning: divide by zero encountered in log\n",
      "  params[1:, i] = np.nan_to_num(np.log(likelihood[:, i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Amazon Sentiment Accuracy train: 0.987500 test: 0.635000\n"
     ]
    }
   ],
   "source": [
    "smoothing=False \n",
    "\n",
    "import lxmls.classifiers.multinomial_naive_bayes as mnbb\n",
    "mnb = mnbb.MultinomialNaiveBayes()\n",
    "params_nb_sc = mnb.train(scr.train_X,scr.train_y, smoothing)\n",
    "y_pred_train = mnb.test(scr.train_X,params_nb_sc)\n",
    "acc_train = mnb.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = mnb.test(scr.test_X,params_nb_sc)\n",
    "acc_test = mnb.evaluate(scr.test_y, y_pred_test)\n",
    "print \"Multinomial Naive Bayes Amazon Sentiment Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 3.\n",
    "Observe that words that were not observed at training time cause problems at test time. Why? To solve this problem, apply a simple add-one smoothing technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideia is to change likelihood as to:\n",
    "    likelihood[:,i]=(1+freq_of_each_word)/ (freq_of_each_word.sum()+n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Amazon Sentiment Accuracy train: 0.974375 test: 0.840000\n"
     ]
    }
   ],
   "source": [
    "smoothing=True \n",
    "\n",
    "import lxmls.classifiers.multinomial_naive_bayes as mnbb\n",
    "mnb = mnbb.MultinomialNaiveBayes()\n",
    "params_nb_sc = mnb.train(scr.train_X,scr.train_y,smoothing)\n",
    "y_pred_train = mnb.test(scr.train_X,params_nb_sc)\n",
    "acc_train = mnb.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = mnb.test(scr.test_X,params_nb_sc)\n",
    "acc_test = mnb.evaluate(scr.test_y, y_pred_test)\n",
    "print \"Multinomial Naive Bayes Amazon Sentiment Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b> Exercise1.2 </b>\n",
    "<br>\n",
    "We provide an implementation of the perceptron algorithm in the class Perceptron (file perceptron.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 1. Run the following commands to generate a simple dataset similar to the one plotted on Figure 1.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.simple_data_set as sds\n",
    "sd = sds.SimpleDataSet(nr_examples=100, g1 = [[-1,-1],1], g2 = [[1,1],1], balance=0.5, split=[0.5,0,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 2. Run the perceptron algorithm on the simple dataset previously generated and report its train and test set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounds: 0 Accuracy: 0.840000\n",
      "Rounds: 1 Accuracy: 0.900000\n",
      "Rounds: 2 Accuracy: 0.940000\n",
      "Rounds: 3 Accuracy: 0.940000\n",
      "Rounds: 4 Accuracy: 0.860000\n",
      "Rounds: 5 Accuracy: 0.940000\n",
      "Rounds: 6 Accuracy: 0.920000\n",
      "Rounds: 7 Accuracy: 0.920000\n",
      "Rounds: 8 Accuracy: 0.920000\n",
      "Rounds: 9 Accuracy: 0.920000\n",
      "Perceptron Simple Dataset Accuracy train: 0.940000 test: 0.840000\n"
     ]
    }
   ],
   "source": [
    "import lxmls.classifiers.perceptron as percc\n",
    "perc = percc.Perceptron()\n",
    "params_perc_sd = perc.train(sd.train_X,sd.train_y)\n",
    "y_pred_train = perc.test(sd.train_X,params_perc_sd)\n",
    "acc_train = perc.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = perc.test(sd.test_X,params_perc_sd)\n",
    "acc_test = perc.evaluate(sd.test_y, y_pred_test)\n",
    "print \"Perceptron Simple Dataset Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 3. Plot the decision boundary found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RitaRamos/anaconda/envs/mypy26/lib/python2.7/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.69314718 -1.69314718]\n",
      " [-1.          1.        ]\n",
      " [-1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#import matplotlib\n",
    "fig,axis = sd.plot_data()\n",
    "fig,axis = sd.add_line(fig,axis,params_perc_sd,\"Perceptron\",\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 4. Run the perceptron algorithm on the Amazon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounds: 0 Accuracy: 0.870000\n",
      "Rounds: 1 Accuracy: 0.940000\n",
      "Rounds: 2 Accuracy: 0.979375\n",
      "Rounds: 3 Accuracy: 0.965625\n",
      "Rounds: 4 Accuracy: 0.989375\n",
      "Rounds: 5 Accuracy: 0.996250\n",
      "Rounds: 6 Accuracy: 0.995000\n",
      "Rounds: 7 Accuracy: 0.999375\n",
      "Rounds: 8 Accuracy: 0.996250\n",
      "Rounds: 9 Accuracy: 0.998125\n",
      "Perceptron Simple Dataset Accuracy train: 0.998750 test: 0.825000\n"
     ]
    }
   ],
   "source": [
    "params_perc_scr = perc.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = perc.test(scr.train_X,params_perc_scr)\n",
    "acc_train = perc.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = perc.test(scr.test_X,params_perc_scr)\n",
    "acc_test = perc.evaluate(scr.test_y, y_pred_test)\n",
    "print \"Perceptron Simple Dataset Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mypy26]",
   "language": "python",
   "name": "conda-env-mypy26-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
