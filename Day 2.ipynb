{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models\n",
    "\n",
    "In this class, we relax the assumption that the data points are independently and identically distributed (i.i.d.) by moving to a scenario of structured prediction, where the inputs are assumed to have temporal or spacial dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.1 - Hidden Markov Models (HMM) </b>\n",
    "<br>\n",
    "\n",
    "Consider a person who is only interested in four activities: walking in the park (walk), shopping (shop), cleaning the apartment (clean) and playing tennis (tennis). Also, consider that the choice of what the person does on a given day is determined exclusively by the weather on that day, which can be either rainy or sunny. Now, supposing that we observe what the person did on a sequence of days, the question is: can we use that information to predict the weather on each of those days? To tackle this problem, we assume that the weather behaves as a discrete Markov chain: the weather on a given day depends only on the weather on the previous day. The entire system can be described as an HMM.\n",
    "For example, assume we are asked to predict the weather conditions on two different sequences of days. During these two sequences, we observed the person performing the following activities:\n",
    "<br>\n",
    "<ul>\n",
    "<li> “walk walk shop clean” </li>\n",
    "<li> “clean walk tennis walk” </li>\n",
    "</ul>\n",
    "-> This will be our test set.\n",
    "<br>\n",
    "<br>\n",
    "Moreover, and in order to train our model, we are given access to three different sequences of days, containing both\n",
    "the activities performed by the person and the weather on those days, namely: \n",
    "<br>\n",
    "<ul>\n",
    "<li>“walk/rainy walk/sunny shop/sunny clean/sunny”</li>\n",
    "<li>“walk/rainy walk/rainy shop/rainy clean/sunny”</li>\n",
    "<li>“walk/sunny shop/sunny shop/sunny clean/sunny”</li>\n",
    "</ul>\n",
    "<br>\n",
    "-> This will be our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load the simple sequence dataset. From the ipython command line create a simple sequence object and look at the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "[walk/rainy walk/sunny shop/sunny clean/sunny , walk/rainy walk/rainy shop/rainy clean/sunny , walk/sunny shop/sunny shop/sunny clean/sunny ]\n",
      "Test dataset:\n",
      "[walk/rainy walk/sunny shop/sunny clean/sunny , clean/sunny walk/sunny tennis/sunny walk/sunny ]\n"
     ]
    }
   ],
   "source": [
    "print \"Train dataset:\\n\", simple.train\n",
    "print \"Test dataset:\\n\", simple.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get in touch with the classes used to store the sequences, you will need this for the next exercise. Note that each label is internally stored as a number. This number can be used as index of an array to store information regarding that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each sequence: walk/rainy walk/sunny shop/sunny clean/sunny  \n",
      "\n",
      "Each sequence: walk/rainy walk/rainy shop/rainy clean/sunny  \n",
      "\n",
      "Each sequence: walk/sunny shop/sunny shop/sunny clean/sunny  \n",
      "\n",
      "\n",
      "\n",
      "Each sequence.x: [0, 0, 1, 2] \n",
      "\n",
      "Each sequence.x: [0, 0, 1, 2] \n",
      "\n",
      "Each sequence.x: [0, 1, 1, 2] \n",
      "\n",
      "\n",
      "\n",
      "Each sequence.y: [0, 1, 1, 1] \n",
      "\n",
      "Each sequence.y: [0, 0, 0, 1] \n",
      "\n",
      "Each sequence.y: [1, 1, 1, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sequence in simple.train.seq_list: \n",
    "    print \"Each sequence:\", sequence, \"\\n\"\n",
    "\n",
    "print \"\\n\"\n",
    "\n",
    "for sequence in simple.train.seq_list: \n",
    "    print \"Each sequence.x:\", sequence.x, \"\\n\"\n",
    "    \n",
    "print \"\\n\"\n",
    "    \n",
    "for sequence in simple.train.seq_list: \n",
    "    print \"Each sequence.y:\", sequence.y, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the observactions correspond internally to: \n",
    "<br>[walk->0, shop->1, clean->2]\n",
    "<br>[rainy->0, sunny->1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.2 - HMM Maximum Likelihood Training</b>\n",
    "<br>\n",
    "\n",
    "The provided function train supervised from the hmm.py file implements the above parameter estimates. Run this function given the simple dataset above and look at the estimated probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Probabilities: [0.66666667 0.33333333] \n",
      "\n",
      "Transition Probabilities: [[0.5   0.   ]\n",
      " [0.5   0.625]] \n",
      "\n",
      "Final Probabilities: [0.    0.375] \n",
      "\n",
      "Emission Probabilities [[0.75  0.25 ]\n",
      " [0.25  0.375]\n",
      " [0.    0.375]\n",
      " [0.    0.   ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Initial Probabilities:\", hmm.initial_probs, \"\\n\"\n",
    "print \"Transition Probabilities:\", hmm.transition_probs, \"\\n\"\n",
    "print \"Final Probabilities:\", hmm.final_probs, \"\\n\"\n",
    "print \"Emission Probabilities\", hmm.emission_probs, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Are they correct? You can also check the variables ending in  counts instead of  probs to see the raw counts (for example, typing hmm.initial counts will show you the raw counts of initial states). How are the counts related to the probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question, we can checks the implementation is correct with the following sanity checks:\n",
    "<ul>\n",
    "<li> Initial Counts: – Should sum to the number of sentences</li>\n",
    "<li> Transition/Final Counts: – Should sum to the number of tokens </li>\n",
    "<li> Emission Counts: – Should sum to the number of tokens </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inial counts are correct: True\n",
      "Transition and Final counts are correct: True\n",
      "Emission counts are correct: True\n"
     ]
    }
   ],
   "source": [
    "number_of_tokens=sum([len(seq) for seq in simple.train])\n",
    "\n",
    "print \"The inial counts are correct:\",  sum(hmm.initial_counts)==len(simple.train.seq_list)\n",
    "print \"Transition and Final counts are correct:\",hmm.transition_counts.sum()+ hmm.final_counts.sum()==number_of_tokens\n",
    "print \"Emission counts are correct:\", hmm.emission_counts.sum()==number_of_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.3 - Decoding a Sequence </b>\n",
    "<br> \n",
    "Convince yourself that the score of a path in the trellis (summing over the scores above) is equivalent to the log-probability log P(X = x, Y = y), as defined in Eq. 2.2. Use the given function compute scores on the first training sequence. Confirm that the values are correct. You should get the same values as presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial scores [-0.40546511 -1.09861229] \n",
      "\n",
      "Transition scores [[[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]] \n",
      "\n",
      "Final scores [       -inf -0.98082925] \n",
      "\n",
      "Emission scores [[-0.28768207 -1.38629436]\n",
      " [-0.28768207 -1.38629436]\n",
      " [-1.38629436 -0.98082925]\n",
      " [       -inf -0.98082925]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lxmls/sequences/hmm.py:188: RuntimeWarning: divide by zero encountered in log\n",
      "  transition_scores[pos-1, :, :] = np.log(self.transition_probs)\n",
      "lxmls/sequences/hmm.py:186: RuntimeWarning: divide by zero encountered in log\n",
      "  emission_scores[pos, :] = np.log(self.emission_probs[sequence.x[pos], :])\n",
      "lxmls/sequences/hmm.py:191: RuntimeWarning: divide by zero encountered in log\n",
      "  final_scores = np.log(self.final_probs)\n"
     ]
    }
   ],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "print \"Initial scores\", initial_scores, \"\\n\"\n",
    "print \"Transition scores\", transition_scores, \"\\n\"\n",
    "print \"Final scores\", final_scores, \"\\n\"\n",
    "print \"Emission scores\", emission_scores, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.4 - Computing in log-domain</b> \n",
    "<br>\n",
    "Look at the module sequences/log domain.py. This module implements a function logsum pair(logx, logy) to add two numbers represented in the log-domain; it returns their sum also represented in the log-domain. The function logsum(logv) sums all components of an array represented in the log-domain. This will be used later in our decoding algorithms. To observe why this is important, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9004691857829754\n",
      "9.900454359630778\n",
      "95.02317525550674\n",
      "inf\n",
      "\n",
      "VS\n",
      "\n",
      "2.9004691857829754\n",
      "9.90045435963078\n",
      "95.02317525550674\n",
      "950.2317518008309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RitaRamos/anaconda/envs/mypy26/lib/python2.7/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(10) \n",
    "\n",
    "print np.log(sum(np.exp(a))) \n",
    "print np.log(sum(np.exp(10*a)))\n",
    "print np.log(sum(np.exp(100*a)))\n",
    "print np.log(sum(np.exp(1000*a)))\n",
    "\n",
    "print \"\\nVS\\n\"\n",
    "from lxmls.sequences.log_domain import *\n",
    "\n",
    "print logsum(a)\n",
    "print logsum(10*a)\n",
    "print logsum(100*a)\n",
    "print logsum(1000*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.5 </b>\n",
    "<br> Run the provided forward-backward algorithm on the first train sequence. Observe that both the forward and the backward passes give the same log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood = -5.068232326005127\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, forward = hmm.decoder.run_forward(initial_scores, transition_scores,\n",
    "    final_scores, emission_scores)\n",
    "print 'Log-Likelihood =', log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood = -5.068232326005126\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, backward = hmm.decoder.run_backward(initial_scores, transition_scores,\n",
    "    final_scores, emission_scores)\n",
    "print 'Log-Likelihood =', log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.6 </b>\n",
    "<br>\n",
    "Compute the node posteriors for the first training sequence (use the provided compute posteriors func- tion), and look at the output. Note that the state posteriors are a proper probability distribution (the lines of the result sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95738152 0.04261848]\n",
      " [0.75281282 0.24718718]\n",
      " [0.26184794 0.73815206]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "state_posteriors, _, _ = hmm.compute_posteriors(initial_scores,transition_scores,final_scores,emission_scores)\n",
    " \n",
    "print state_posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.7 </b>\n",
    "<br>\n",
    "Run the posterior decode on the first test sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 0: walk/rainy walk/rainy shop/sunny clean/sunny \n",
      "Truth test 0:      walk/rainy walk/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0])\n",
    "print \"Prediction test 0:\", y_pred\n",
    "print \"Truth test 0:     \", simple.test.seq_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(y_seq, predicted_y_seq , number_of_classes):\n",
    "    accuracy=0\n",
    "    precision=0\n",
    "    recall=0\n",
    "    evaluations=0\n",
    "    for class_i in number_of_classes:\n",
    "        elements_correct= sum(1 for i in range(len(y_seq)) if predicted_y_seq[i]==class_i and predicted_y_seq[i]==y_seq[i])\n",
    "        if elements_correct != 0:\n",
    "            accuracy+=elements_correct #accuracy is the nº of elements correctly predicted of each class\n",
    "            precision+=elements_correct/(predicted_y_seq == class_i).sum(0) #precision of each class\n",
    "            recall+=elements_correct/ (y_seq).count(class_i)                #recall of class y\n",
    "            evaluations+=1\n",
    "    print \"\\nAccuracy:\", accuracy, \"\\n\"\n",
    "    if evaluations !=0:\n",
    "        print \"Precision:\", precision/evaluations, \"\\n\"\n",
    "        print \"Recall:\", recall/evaluations,\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 3 \n",
      "\n",
      "Precision: 0.75 \n",
      "\n",
      "Recall: 0.833333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(simple.test.seq_list[0].y, y_pred.y, simple.y_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do the same for the second test sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 1: clean/rainy walk/rainy tennis/rainy walk/rainy \n",
      "Truth test 1:      clean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "\n",
      "Accuracy: 0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lxmls/sequences/sequence_classifier.py:79: RuntimeWarning: invalid value encountered in subtract\n",
      "  state_posteriors[pos, :] -= log_likelihood\n",
      "lxmls/sequences/sequence_classifier.py:92: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  transition_posteriors[pos, state, prev_state] -= log_likelihood\n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "print \"Prediction test 1:\", y_pred\n",
    "print \"Truth test 1:     \", simple.test.seq_list[1]\n",
    "\n",
    "evaluate(simple.test.seq_list[1].y, y_pred.y, simple.y_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is wrong? Note the observations for the second test sequence: the observation tennis was never seen at training time, so the probability for it will be zero (no matter what state). This will make all possible state sequences have zero probability. As seen in the previous lecture, this is a problem with generative models, which can be corrected using smoothing (among other options).\n",
    "\n",
    "> <br> Change the train supervised method to add smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmm.train_supervised(simple.train, smoothing=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing Prediction test 0: walk/rainy walk/rainy shop/sunny clean/sunny \n",
      "Smoothing Truth test 0:      walk/rainy walk/sunny shop/sunny clean/sunny \n",
      "\n",
      "Accuracy: 3 \n",
      "\n",
      "Precision: 0.75 \n",
      "\n",
      "Recall: 0.833333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0])\n",
    "print \"Smoothing Prediction test 0:\", y_pred\n",
    "print \"Smoothing Truth test 0:     \", simple.test.seq_list[0]\n",
    "evaluate(simple.test.seq_list[0].y, y_pred.y, simple.y_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing Prediction test 1: clean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "Smoothing Truth test 1:      clean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "\n",
      "Accuracy: 4 \n",
      "\n",
      "Precision: 1.0 \n",
      "\n",
      "Recall: 1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "print \"Smoothing Prediction test 1:\", y_pred\n",
    "print \"Smoothing Truth test 1:     \", simple.test.seq_list[1]\n",
    "\n",
    "evaluate(simple.test.seq_list[1].y, y_pred.y, simple.y_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.8 - Viterbi </b>\n",
    "<br>\n",
    "Implement a method for performing Viterbi decoding in file sequence classification decoder.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#My implementation in the class decoder.py\n",
    "\n",
    "def run_viterbi(self, initial_scores, transition_scores, final_scores, emission_scores):\n",
    "\n",
    "    length = np.size(emission_scores, 0)  # Length of the sequence.\n",
    "    num_states = np.size(initial_scores)  # Number of states.\n",
    "\n",
    "    # Variables storing the Viterbi scores.\n",
    "    viterbi_scores = np.zeros([length, num_states]) + logzero()\n",
    "\n",
    "    # Variables storing the paths to backtrack.\n",
    "    viterbi_paths = -np.ones([length, num_states], dtype=int)\n",
    "\n",
    "    # Most likely sequence.\n",
    "    best_path = -np.ones(length, dtype=int)\n",
    "\n",
    "    # Initialization.\n",
    "    viterbi_scores[0, :] = emission_scores[0, :] + initial_scores\n",
    "\n",
    "    # Viterbi loop.\n",
    "    for pos in xrange(1, length):\n",
    "        for current_state in xrange(num_states):\n",
    "\n",
    "            viterbi_scores[pos, current_state]= np.max(viterbi_scores[pos-1, :] + transition_scores[pos-1, current_state, :])\n",
    "            viterbi_scores[pos, current_state]+=emission_scores[pos, current_state]\n",
    "            viterbi_paths[pos, current_state] = np.argmax(viterbi_scores[pos-1, :] + transition_scores[pos-1, current_state, :])\n",
    "\n",
    "    # Termination.\n",
    "    best_score = np.max(viterbi_scores[length-1, :] + final_scores)\n",
    "\n",
    "    #best state\n",
    "    best_path[-1]=np.argmax(viterbi_scores[length-1, :] + final_scores) \n",
    "\n",
    "    for i in xrange(length-2,-1,-1):\n",
    "        best_path[i]=viterbi_paths[i+1, best_path[i+1]]\n",
    "\n",
    "    return best_path, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test your method on both test sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding Prediction test 0 with smoothing:\n",
      "walk/rainy walk/rainy shop/sunny clean/sunny  -6.020501246982869 \n",
      "\n",
      "Truth test 0:\n",
      "walk/rainy walk/sunny shop/sunny clean/sunny  \n",
      "\n",
      "\n",
      "Accuracy: 3 \n",
      "\n",
      "Precision: 0.75 \n",
      "\n",
      "Recall: 0.833333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hmm.train_supervised(simple.train, smoothing=0.1)\n",
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[0])\n",
    "print \"Viterbi decoding Prediction test 0 with smoothing:\\n\", y_pred, score, \"\\n\"\n",
    "print \"Truth test 0:\\n\", simple.test.seq_list[0], \"\\n\"\n",
    "evaluate(simple.test.seq_list[0].y, y_pred.y, simple.y_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding Prediction test 1 with smoothing:\n",
      "clean/sunny walk/sunny tennis/sunny walk/sunny  -11.713974073970887 \n",
      "\n",
      "Truth test 1:\n",
      "clean/sunny walk/sunny tennis/sunny walk/sunny  \n",
      "\n",
      "\n",
      "Accuracy: 4 \n",
      "\n",
      "Precision: 1.0 \n",
      "\n",
      "Recall: 1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[1])\n",
    "print \"Viterbi decoding Prediction test 1 with smoothing:\\n\", y_pred, score, \"\\n\"\n",
    "print \"Truth test 1:\\n\", simple.test.seq_list[1],  \"\\n\"\n",
    "evaluate(simple.test.seq_list[1].y, y_pred.y, simple.y_dict.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the results with the ones given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Applying viterbi with smoothing had the same results of applying posterior decoding with smoothing, in this particular case. Althought they had the same result for this case in particular, it should be mentioned that, in general, posterior decoding has the downside of not guaranting a valid setence, since it picks the highest state posterior for each position in the sequence; whereas the viterbi picks the best global hidden state sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part-of-Speech Tagging (POS)</b>\n",
    "<br>\n",
    "Part-of-Speech (PoS) tagging is one of the most important NLP tasks. The task is to assign each word a grammatical category, or Part-of-Speech, i.e. noun, verb, adjective,... Recalling the defined notation, Σ is a vocabulary of word types, and Λ is the set of Part-of-Speech tags.\n",
    "In English, using the Penn Treebank (PTB) corpus (Marcus et al., 1993), the current state of the art for part of speech tagging is around 97% for a variety of methods.\n",
    "In the rest of this class we will use a subset of the PTB corpus, but instead of using the original 45 tags we will use a reduced tag set of 12 tags, to make the algorithms faster for the class. In this task, x is a sentence (i.e., a sequence of word tokens) and y is the sequence of possible PoS tags.\n",
    "<br>\n",
    "<br>\n",
    "The first step is to load the corpus. We will start by loading 1000 sentences for training and 1000 sentences both for development and testing. Then we train the HMM model by maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c210790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "corpus = pcc.PostagCorpus()\n",
    "train_seq = corpus.read_sequence_list_conll(\"data/train-02-21.conll\",max_sent_len=15,\n",
    "    max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(\"data/test-23.conll\",max_sent_len=15,\n",
    "    max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(\"data/dev-22.conll\",max_sent_len=15,max_nr_sent\n",
    "    =1000)\n",
    "hmm = hmmc.HMM(corpus.word_dict, corpus.tag_dict)\n",
    "hmm.train_supervised(train_seq)\n",
    "hmm.print_transition_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The tag_ dict:\n",
      "{'adv': 8, 'verb': 6, 'noun': 0, 'adp': 1, 'pron': 9, 'det': 2, '.': 4, 'prt': 5, 'num': 3, 'x': 11, 'conj': 7, 'adj': 10}\n",
      "\n",
      "Example of train_sequences:\n",
      "[Ms./noun Haag/noun plays/verb Elianti/noun ./. , The/det luxury/noun auto/noun maker/noun last/adj year/noun sold/verb 1,214/num cars/noun in/adp the/det U.S./noun , The/det new/adj rate/noun will/verb be/verb payable/adj Feb./noun 15/num ./. , A/det record/noun date/noun has/verb n't/adv been/verb set/verb ./. , ``/. Apparently/adv the/det commission/noun did/verb not/adv really/adv believe/verb in/adp this/det ideal/noun ./. ''/. ]\n"
     ]
    }
   ],
   "source": [
    "print \"\\nThe tag_ dict:\\n\", corpus.tag_dict\n",
    "print \"\\nExample of train_sequences:\\n\", train_seq[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.9 Test the model using both posterior decoding and Viterbi decoding on both the train and test set, using the methods in class HMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: Posterior Decode 0.985, Viterbi Decode: 0.985\n"
     ]
    }
   ],
   "source": [
    "viterbi_pred_train = hmm.viterbi_decode_corpus(train_seq)\n",
    "posterior_pred_train = hmm.posterior_decode_corpus(train_seq)\n",
    "eval_viterbi_train = hmm.evaluate_corpus(train_seq, viterbi_pred_train)\n",
    "eval_posterior_train = hmm.evaluate_corpus(train_seq, posterior_pred_train)\n",
    "print \"Train Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(eval_posterior_train,eval_viterbi_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: Posterior Decode 0.350, Viterbi Decode: 0.509\n"
     ]
    }
   ],
   "source": [
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq)\n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq,viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq,posterior_pred_test)\n",
    "print \"Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(eval_posterior_test,eval_viterbi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a decrease accuracy from the training dataset to the test dataset, which is probably due to the fact that there were words that were not observed at training, leading to zero probabilities (and consequently to misclassifications). To solve this problem we should use smoothing, as we have already did in previous exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remake the previous exercise but now train the HMM using smoothing. Try different values (0,0.1,0.01,1) and report the results on the train and development set. (Use function pick best smoothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing 10.000000 --  Train Set Accuracy: Posterior Decode 0.731, Viterbi Decode: 0.691\n",
      "Smoothing 10.000000 -- Test Set Accuracy: Posterior Decode 0.712, Viterbi Decode: 0.675\n",
      "Smoothing 1.000000 --  Train Set Accuracy: Posterior Decode 0.887, Viterbi Decode: 0.865\n",
      "Smoothing 1.000000 -- Test Set Accuracy: Posterior Decode 0.818, Viterbi Decode: 0.792\n",
      "Smoothing 0.100000 --  Train Set Accuracy: Posterior Decode 0.968, Viterbi Decode: 0.965\n",
      "Smoothing 0.100000 -- Test Set Accuracy: Posterior Decode 0.851, Viterbi Decode: 0.842\n",
      "Smoothing 0.000000 --  Train Set Accuracy: Posterior Decode 0.985, Viterbi Decode: 0.985\n",
      "Smoothing 0.000000 -- Test Set Accuracy: Posterior Decode 0.370, Viterbi Decode: 0.526\n"
     ]
    }
   ],
   "source": [
    "best_smoothing = hmm.pick_best_smoothing(train_seq, dev_seq, [10,1,0.1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Smoothing 0.100000 -- Test Set Accuracy: Posterior Decode 0.837, Viterbi Decode: 0.827\n"
     ]
    }
   ],
   "source": [
    "hmm.train_supervised(train_seq, smoothing=best_smoothing)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq)\n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)\n",
    "print \"Best Smoothing %f -- Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(best_smoothing,eval_posterior_test,eval_viterbi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Perform some error analysis to understand were the errors are coming from. You can start by visualizing the confusion matrix (true tags vs predicted tags). You should get something like what is shown in Figure 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c9c5810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lxmls.sequences.confusion_matrix as cm\n",
    "import matplotlib.pyplot as plt\n",
    "confusion_matrix = cm.build_confusion_matrix(test_seq.seq_list, viterbi_pred_test, len(\n",
    "corpus.tag_dict), hmm.get_num_states())\n",
    "cm.plot_confusion_bar_graph(confusion_matrix, corpus.tag_dict, xrange(hmm.get_num_states\n",
    "    ()), 'Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Unsupervised Learning of HMMs </b>\n",
    "\n",
    "We next address the problem of unsupervised learning. In this setting, we are not given any labeled data; all we get to see is a set of natural language sentences. The underlying question is:\n",
    "Can we learn something from raw text?\n",
    "This task is challenging, since the process by which linguistic structures are generated is not always clear; and even when it is, it is typically too complex to be formally expressed. Nevertheless, unsupervised learning hasbeenappliedtoawiderangeofNLPtasks,suchas:Part-of-SpeechInduction(Schu ̈tze,1995;Merialdo, 1994; Clark, 2003), Dependency Grammar Induction (Klein and Manning, 2004; Smith and Eisner, 2006), Con- stituency Grammar Induction (Klein and Manning, 2004), Statistical Word Alignments (Brown et al., 1993) and Anaphora Resolution (Charniak and Elsner, 2009), just to name a few.\n",
    "Different motivations have pushed research in this area. From both a linguistic and cognitive point of view, unsupervised learning is useful as a tool to study language acquisition. From a machine learning point of view, unsupervised learning is a fertile ground for testing new learning methods, where significant improvements can yet be made. From a more pragmatic perspective, unsupervised learning is required since annotated corpora is a scarce resource for different reasons. Independently of the reason, unsupervised learning is an increasingly active field of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.10 </b>\n",
    "<br> Implement the method to update the counts given the state and transition posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  def update_counts(self, sequence, state_posteriors, transition_posteriors):\n",
    "        \"\"\" Used in the E-step in EM.\"\"\"\n",
    "        \n",
    "        states = self.get_num_states()\n",
    "        N = len(sequence.x) \n",
    "\n",
    "        for y in xrange(states):\n",
    "            self.initial_counts[y] += state_posteriors[0, y]\n",
    "            \n",
    "        for y in xrange(states):\n",
    "            self.final_counts[y] += state_posteriors[N-1, y]\n",
    "            \n",
    "        for i in xrange(length):\n",
    "            x = sequence.x[i]\n",
    "            for state in xrange(num_states):\n",
    "                self.emission_counts[x, state] += state_posteriors[i, state]\n",
    "                if i > 0: \n",
    "                    for state_before in xrange(num_states):\n",
    "                        self.transition_counts[state, state_before] += transition_posteriors[i-1, state, state_before]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You now have all the pieces to implement the EM algorithm. Look at the code for EM algorithm in file sequences/hmm.py and check it for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_EM(self, dataset, smoothing=0, num_epochs=10, evaluate=True): \n",
    "    self.initialize_random()\n",
    "    if evaluate:\n",
    "        acc = self.evaluate_EM(dataset)\n",
    "        print \"Initial accuracy: %f\"%(acc)\n",
    "    for t in xrange(1, num_epochs): #E-Step\n",
    "        total_log_likelihood = 0.0\n",
    "        self.clear_counts(smoothing)\n",
    "    for sequence in dataset.seq_list:\n",
    "        # Compute scores given the observation sequence.\n",
    "        initial_scores, transition_scores, final_scores, emission_scores = \\\n",
    "                    self.compute_scores(sequence)\n",
    "        state_posteriors, transition_posteriors, log_likelihood = \\\n",
    "            self.compute_posteriors(initial_scores,\n",
    "                                    transition_scores,\n",
    "                                    final_scores,\n",
    "                                    emission_scores)\n",
    "        self.update_counts(sequence, state_posteriors, transition_posteriors)\n",
    "    total_log_likelihood += log_likelihood\n",
    "    print \"Iter: %i Log Likelihood: %f\"%(t, total_log_likelihood) #M-Step\n",
    "    self.compute_parameters()\n",
    "    if evaluate:\n",
    "        ### Evaluate accuracy at this iteration\n",
    "        acc = self.evaluate_EM(dataset)\n",
    "        print \"Iter: %i Accuracy: %f\"%(t,acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.11 </b>\n",
    "<br>\n",
    "Run 20 epochs of the EM algorithm for part of speech induction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy: 0.323680\n",
      "Iter: 1 Log Likelihood: -101732.453384\n",
      "Iter: 1 Accuracy: 0.364465\n",
      "Iter: 2 Log Likelihood: -78078.332344\n",
      "Iter: 2 Accuracy: 0.381802\n",
      "Iter: 3 Log Likelihood: -77890.749710\n",
      "Iter: 3 Accuracy: 0.389919\n",
      "Iter: 4 Log Likelihood: -77323.487135\n",
      "Iter: 4 Accuracy: 0.391823\n",
      "Iter: 5 Log Likelihood: -76066.118912\n",
      "Iter: 5 Accuracy: 0.391622\n",
      "Iter: 6 Log Likelihood: -74361.781759\n",
      "Iter: 6 Accuracy: 0.389718\n",
      "Iter: 7 Log Likelihood: -72538.030601\n",
      "Iter: 7 Accuracy: 0.389017\n",
      "Iter: 8 Log Likelihood: -70645.317981\n",
      "Iter: 8 Accuracy: 0.386111\n",
      "Iter: 9 Log Likelihood: -68879.994782\n",
      "Iter: 9 Accuracy: 0.386111\n",
      "Iter: 10 Log Likelihood: -67545.008536\n",
      "Iter: 10 Accuracy: 0.385510\n",
      "Iter: 11 Log Likelihood: -66720.621561\n",
      "Iter: 11 Accuracy: 0.385510\n",
      "Iter: 12 Log Likelihood: -66274.995826\n",
      "Iter: 12 Accuracy: 0.385409\n",
      "Iter: 13 Log Likelihood: -66002.604459\n",
      "Iter: 13 Accuracy: 0.385309\n",
      "Iter: 14 Log Likelihood: -65794.154720\n",
      "Iter: 14 Accuracy: 0.385309\n",
      "Iter: 15 Log Likelihood: -65657.796665\n",
      "Iter: 15 Accuracy: 0.385309\n",
      "Iter: 16 Log Likelihood: -65600.621477\n",
      "Iter: 16 Accuracy: 0.385309\n",
      "Iter: 17 Log Likelihood: -65576.104535\n",
      "Iter: 17 Accuracy: 0.385309\n",
      "Iter: 18 Log Likelihood: -65563.788151\n",
      "Iter: 18 Accuracy: 0.385309\n",
      "Iter: 19 Log Likelihood: -65558.054545\n",
      "Iter: 19 Accuracy: 0.385309\n"
     ]
    }
   ],
   "source": [
    "hmm.train_EM(train_seq, 0.1, 20, evaluate=True)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq)\n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c23bf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = cm.build_confusion_matrix(test_seq.seq_list, viterbi_pred_test, len(corpus.tag_dict), hmm.get_num_states())\n",
    "cm.plot_confusion_bar_graph(confusion_matrix, corpus.tag_dict, xrange(hmm.get_num_states()), 'Confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mypy26]",
   "language": "python",
   "name": "conda-env-mypy26-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
