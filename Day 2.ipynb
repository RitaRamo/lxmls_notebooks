{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models\n",
    "\n",
    "In this class, we relax the assumption that the data points are independently and identically distributed (i.i.d.) by moving to a scenario of structured prediction, where the inputs are assumed to have temporal or spacial dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.1 - Hidden Markov Models (HMM) </b>\n",
    "<br>\n",
    "\n",
    "Consider a person who is only interested in four activities: walking in the park (walk), shopping (shop), cleaning the apartment (clean) and playing tennis (tennis). Also, consider that the choice of what the person does on a given day is determined exclusively by the weather on that day, which can be either rainy or sunny. Now, supposing that we observe what the person did on a sequence of days, the question is: can we use that information to predict the weather on each of those days? To tackle this problem, we assume that the weather behaves as a discrete Markov chain: the weather on a given day depends only on the weather on the previous day. The entire system can be described as an HMM.\n",
    "For example, assume we are asked to predict the weather conditions on two different sequences of days. During these two sequences, we observed the person performing the following activities:\n",
    "<br>\n",
    "<ul>\n",
    "<li> “walk walk shop clean” </li>\n",
    "<li> “clean walk tennis walk” </li>\n",
    "</ul>\n",
    "-> This will be our test set.\n",
    "<br>\n",
    "<br>\n",
    "Moreover, and in order to train our model, we are given access to three different sequences of days, containing both\n",
    "the activities performed by the person and the weather on those days, namely: \n",
    "<br>\n",
    "<ul>\n",
    "<li>“walk/rainy walk/sunny shop/sunny clean/sunny”</li>\n",
    "<li>“walk/rainy walk/rainy shop/rainy clean/sunny”</li>\n",
    "<li>“walk/sunny shop/sunny shop/sunny clean/sunny”</li>\n",
    "</ul>\n",
    "<br>\n",
    "-> This will be our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load the simple sequence dataset. From the ipython command line create a simple sequence object and look at the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "[walk/rainy walk/sunny shop/sunny clean/sunny , walk/rainy walk/rainy shop/rainy clean/sunny , walk/sunny shop/sunny shop/sunny clean/sunny ]\n",
      "Test dataset:\n",
      "[walk/rainy walk/sunny shop/sunny clean/sunny , clean/sunny walk/sunny tennis/sunny walk/sunny ]\n"
     ]
    }
   ],
   "source": [
    "print \"Train dataset:\\n\", simple.train\n",
    "print \"Test dataset:\\n\", simple.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get in touch with the classes used to store the sequences, you will need this for the next exercise. Note that each label is internally stored as a number. This number can be used as index of an array to store information regarding that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each sequence: walk/rainy walk/sunny shop/sunny clean/sunny  \n",
      "\n",
      "Each sequence: walk/rainy walk/rainy shop/rainy clean/sunny  \n",
      "\n",
      "Each sequence: walk/sunny shop/sunny shop/sunny clean/sunny  \n",
      "\n",
      "\n",
      "\n",
      "Each sequence.x: [0, 0, 1, 2] \n",
      "\n",
      "Each sequence.x: [0, 0, 1, 2] \n",
      "\n",
      "Each sequence.x: [0, 1, 1, 2] \n",
      "\n",
      "\n",
      "\n",
      "Each sequence.y: [0, 1, 1, 1] \n",
      "\n",
      "Each sequence.y: [0, 0, 0, 1] \n",
      "\n",
      "Each sequence.y: [1, 1, 1, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sequence in simple.train.seq_list: \n",
    "    print \"Each sequence:\", sequence, \"\\n\"\n",
    "\n",
    "print \"\\n\"\n",
    "\n",
    "for sequence in simple.train.seq_list: \n",
    "    print \"Each sequence.x:\", sequence.x, \"\\n\"\n",
    "    \n",
    "print \"\\n\"\n",
    "    \n",
    "for sequence in simple.train.seq_list: \n",
    "    print \"Each sequence.y:\", sequence.y, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the observactions correspond internally to: \n",
    "<br>[walk->0, shop->1, clean->2]\n",
    "<br>[rainy->0, sunny->1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.2 - HMM Maximum Likelihood Training</b>\n",
    "<br>\n",
    "\n",
    "The provided function train supervised from the hmm.py file implements the above parameter estimates. Run this function given the simple dataset above and look at the estimated probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Probabilities: [0.66666667 0.33333333] \n",
      "\n",
      "Transition Probabilities: [[0.5   0.   ]\n",
      " [0.5   0.625]] \n",
      "\n",
      "Final Probabilities: [0.    0.375] \n",
      "\n",
      "Emission Probabilities [[0.75  0.25 ]\n",
      " [0.25  0.375]\n",
      " [0.    0.375]\n",
      " [0.    0.   ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Initial Probabilities:\", hmm.initial_probs, \"\\n\"\n",
    "print \"Transition Probabilities:\", hmm.transition_probs, \"\\n\"\n",
    "print \"Final Probabilities:\", hmm.final_probs, \"\\n\"\n",
    "print \"Emission Probabilities\", hmm.emission_probs, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Are they correct? You can also check the variables ending in  counts instead of  probs to see the raw counts (for example, typing hmm.initial counts will show you the raw counts of initial states). How are the counts related to the probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question, we can checks the implementation is correct with the following sanity checks:\n",
    "<ul>\n",
    "<li> Initial Counts: – Should sum to the number of sentences</li>\n",
    "<li> Transition/Final Counts: – Should sum to the number of tokens </li>\n",
    "<li> Emission Counts: – Should sum to the number of tokens </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inial counts are correct: True\n",
      "Transition and Final counts are correct: True\n",
      "Emission counts are correct: True\n"
     ]
    }
   ],
   "source": [
    "number_of_tokens=sum([len(seq) for seq in simple.train])\n",
    "\n",
    "print \"The inial counts are correct:\",  sum(hmm.initial_counts)==len(simple.train.seq_list)\n",
    "print \"Transition and Final counts are correct:\",hmm.transition_counts.sum()+ hmm.final_counts.sum()==number_of_tokens\n",
    "print \"Emission counts are correct:\", hmm.emission_counts.sum()==number_of_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.3 - Decoding a Sequence (Viterbi) </b>\n",
    "<br> \n",
    "Convince yourself that the score of a path in the trellis (summing over the scores above) is equivalent to the log-probability log P(X = x, Y = y), as defined in Eq. 2.2. Use the given function compute scores on the first training sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial scores [-0.40546511 -1.09861229] \n",
      "\n",
      "Transition scores [[[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]] \n",
      "\n",
      "Final scores [       -inf -0.98082925] \n",
      "\n",
      "Emission scores [[-0.28768207 -1.38629436]\n",
      " [-0.28768207 -1.38629436]\n",
      " [-1.38629436 -0.98082925]\n",
      " [       -inf -0.98082925]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "print \"Initial scores\", initial_scores, \"\\n\"\n",
    "print \"Transition scores\", transition_scores, \"\\n\"\n",
    "print \"Final scores\", final_scores, \"\\n\"\n",
    "print \"Emission scores\", emission_scores, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Confirm that the values are correct. You should get the same values as presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.4 - Computing in log-domain</b> \n",
    "<br>\n",
    "Look at the module sequences/log domain.py. This module implements a function logsum pair(logx, logy) to add two numbers represented in the log-domain; it returns their sum also represented in the log-domain. The function logsum(logv) sums all components of an array represented in the log-domain. This will be used later in our decoding algorithms. To observe why this is important, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.895267533353047\n",
      "10.00652783229396\n",
      "91.99316436690594\n",
      "inf\n",
      "\n",
      "VS\n",
      "\n",
      "2.895267533353047\n",
      "10.006527832293958\n",
      "91.99316436690594\n",
      "919.9063921705749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RitaRamos/anaconda/envs/mypy26/lib/python2.7/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(10) \n",
    "\n",
    "print np.log(sum(np.exp(a))) \n",
    "print np.log(sum(np.exp(10*a)))\n",
    "print np.log(sum(np.exp(100*a)))\n",
    "print np.log(sum(np.exp(1000*a)))\n",
    "\n",
    "print \"\\nVS\\n\"\n",
    "from lxmls.sequences.log_domain import *\n",
    "\n",
    "print logsum(a)\n",
    "print logsum(10*a)\n",
    "print logsum(100*a)\n",
    "print logsum(1000*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mypy26]",
   "language": "python",
   "name": "conda-env-mypy26-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
