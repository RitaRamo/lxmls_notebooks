{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models\n",
    "\n",
    "In this class, we relax the assumption that the data points are independently and identically distributed (i.i.d.) by moving to a scenario of structured prediction, where the inputs are assumed to have temporal or spacial dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.1 - Hidden Markov Models (HMM) </b>\n",
    "<br>\n",
    "\n",
    "Consider a person who is only interested in four activities: walking in the park (walk), shopping (shop), cleaning the apartment (clean) and playing tennis (tennis). Also, consider that the choice of what the person does on a given day is determined exclusively by the weather on that day, which can be either rainy or sunny. Now, supposing that we observe what the person did on a sequence of days, the question is: can we use that information to predict the weather on each of those days? To tackle this problem, we assume that the weather behaves as a discrete Markov chain: the weather on a given day depends only on the weather on the previous day. The entire system can be described as an HMM.\n",
    "For example, assume we are asked to predict the weather conditions on two different sequences of days. During these two sequences, we observed the person performing the following activities:\n",
    "<br>\n",
    "<ul>\n",
    "<li> “walk walk shop clean” </li>\n",
    "<li> “clean walk tennis walk” </li>\n",
    "</ul>\n",
    "-> This will be our test set.\n",
    "<br>\n",
    "<br>\n",
    "Moreover, and in order to train our model, we are given access to three different sequences of days, containing both\n",
    "the activities performed by the person and the weather on those days, namely: \n",
    "<br>\n",
    "<ul>\n",
    "<li>“walk/rainy walk/sunny shop/sunny clean/sunny”</li>\n",
    "<li>“walk/rainy walk/rainy shop/rainy clean/sunny”</li>\n",
    "<li>“walk/sunny shop/sunny shop/sunny clean/sunny”</li>\n",
    "</ul>\n",
    "<br>\n",
    "-> This will be our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load the simple sequence dataset. From the ipython command line create a simple sequence object and look at the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "[walk/rainy walk/sunny shop/sunny clean/sunny , walk/rainy walk/rainy shop/rainy clean/sunny , walk/sunny shop/sunny shop/sunny clean/sunny ]\n",
      "Test dataset:\n",
      "[walk/rainy walk/sunny shop/sunny clean/sunny , clean/sunny walk/sunny tennis/sunny walk/sunny ]\n"
     ]
    }
   ],
   "source": [
    "print \"Train dataset:\\n\", simple.train\n",
    "print \"Test dataset:\\n\", simple.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get in touch with the classes used to store the sequences, you will need this for the next exercise. Note that each label is internally stored as a number. This number can be used as index of an array to store information regarding that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each sequence: walk/rainy walk/sunny shop/sunny clean/sunny  \n",
      "\n",
      "Each sequence: walk/rainy walk/rainy shop/rainy clean/sunny  \n",
      "\n",
      "Each sequence: walk/sunny shop/sunny shop/sunny clean/sunny  \n",
      "\n",
      "\n",
      "\n",
      "Each sequence.x: [0, 0, 1, 2] \n",
      "\n",
      "Each sequence.x: [0, 0, 1, 2] \n",
      "\n",
      "Each sequence.x: [0, 1, 1, 2] \n",
      "\n",
      "\n",
      "\n",
      "Each sequence.y: [0, 1, 1, 1] \n",
      "\n",
      "Each sequence.y: [0, 0, 0, 1] \n",
      "\n",
      "Each sequence.y: [1, 1, 1, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sequence in simple.train.seq_list: \n",
    "    print \"Each sequence:\", sequence, \"\\n\"\n",
    "\n",
    "print \"\\n\"\n",
    "\n",
    "for sequence in simple.train.seq_list: \n",
    "    print \"Each sequence.x:\", sequence.x, \"\\n\"\n",
    "    \n",
    "print \"\\n\"\n",
    "    \n",
    "for sequence in simple.train.seq_list: \n",
    "    print \"Each sequence.y:\", sequence.y, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the observactions correspond internally to: \n",
    "<br>[walk->0, shop->1, clean->2]\n",
    "<br>[rainy->0, sunny->1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.2 - HMM Maximum Likelihood Training</b>\n",
    "<br>\n",
    "\n",
    "The provided function train supervised from the hmm.py file implements the above parameter estimates. Run this function given the simple dataset above and look at the estimated probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Probabilities: [0.66666667 0.33333333] \n",
      "\n",
      "Transition Probabilities: [[0.5   0.   ]\n",
      " [0.5   0.625]] \n",
      "\n",
      "Final Probabilities: [0.    0.375] \n",
      "\n",
      "Emission Probabilities [[0.75  0.25 ]\n",
      " [0.25  0.375]\n",
      " [0.    0.375]\n",
      " [0.    0.   ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Initial Probabilities:\", hmm.initial_probs, \"\\n\"\n",
    "print \"Transition Probabilities:\", hmm.transition_probs, \"\\n\"\n",
    "print \"Final Probabilities:\", hmm.final_probs, \"\\n\"\n",
    "print \"Emission Probabilities\", hmm.emission_probs, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Are they correct? You can also check the variables ending in  counts instead of  probs to see the raw counts (for example, typing hmm.initial counts will show you the raw counts of initial states). How are the counts related to the probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question, we can checks the implementation is correct with the following sanity checks:\n",
    "<ul>\n",
    "<li> Initial Counts: – Should sum to the number of sentences</li>\n",
    "<li> Transition/Final Counts: – Should sum to the number of tokens </li>\n",
    "<li> Emission Counts: – Should sum to the number of tokens </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inial counts are correct: True\n",
      "Transition and Final counts are correct: True\n",
      "Emission counts are correct: True\n"
     ]
    }
   ],
   "source": [
    "number_of_tokens=sum([len(seq) for seq in simple.train])\n",
    "\n",
    "print \"The inial counts are correct:\",  sum(hmm.initial_counts)==len(simple.train.seq_list)\n",
    "print \"Transition and Final counts are correct:\",hmm.transition_counts.sum()+ hmm.final_counts.sum()==number_of_tokens\n",
    "print \"Emission counts are correct:\", hmm.emission_counts.sum()==number_of_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.3 - Decoding a Sequence </b>\n",
    "<br> \n",
    "Convince yourself that the score of a path in the trellis (summing over the scores above) is equivalent to the log-probability log P(X = x, Y = y), as defined in Eq. 2.2. Use the given function compute scores on the first training sequence. Confirm that the values are correct. You should get the same values as presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial scores [-0.40546511 -1.09861229] \n",
      "\n",
      "Transition scores [[[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]] \n",
      "\n",
      "Final scores [       -inf -0.98082925] \n",
      "\n",
      "Emission scores [[-0.28768207 -1.38629436]\n",
      " [-0.28768207 -1.38629436]\n",
      " [-1.38629436 -0.98082925]\n",
      " [       -inf -0.98082925]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lxmls/sequences/hmm.py:179: RuntimeWarning: divide by zero encountered in log\n",
      "  transition_scores[pos-1, :, :] = np.log(self.transition_probs)\n",
      "lxmls/sequences/hmm.py:177: RuntimeWarning: divide by zero encountered in log\n",
      "  emission_scores[pos, :] = np.log(self.emission_probs[sequence.x[pos], :])\n",
      "lxmls/sequences/hmm.py:182: RuntimeWarning: divide by zero encountered in log\n",
      "  final_scores = np.log(self.final_probs)\n"
     ]
    }
   ],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "print \"Initial scores\", initial_scores, \"\\n\"\n",
    "print \"Transition scores\", transition_scores, \"\\n\"\n",
    "print \"Final scores\", final_scores, \"\\n\"\n",
    "print \"Emission scores\", emission_scores, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.4 - Computing in log-domain</b> \n",
    "<br>\n",
    "Look at the module sequences/log domain.py. This module implements a function logsum pair(logx, logy) to add two numbers represented in the log-domain; it returns their sum also represented in the log-domain. The function logsum(logv) sums all components of an array represented in the log-domain. This will be used later in our decoding algorithms. To observe why this is important, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.607942313490166\n",
      "5.991010474846291\n",
      "47.77049502782578\n",
      "477.45974653821446\n",
      "\n",
      "VS\n",
      "\n",
      "2.6079423134901663\n",
      "5.991010474846291\n",
      "47.77049502782578\n",
      "477.45974653821446\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(10) \n",
    "\n",
    "print np.log(sum(np.exp(a))) \n",
    "print np.log(sum(np.exp(10*a)))\n",
    "print np.log(sum(np.exp(100*a)))\n",
    "print np.log(sum(np.exp(1000*a)))\n",
    "\n",
    "print \"\\nVS\\n\"\n",
    "from lxmls.sequences.log_domain import *\n",
    "\n",
    "print logsum(a)\n",
    "print logsum(10*a)\n",
    "print logsum(100*a)\n",
    "print logsum(1000*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.5 </b>\n",
    "<br> Run the provided forward-backward algorithm on the first train sequence. Observe that both the forward and the backward passes give the same log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood = -5.068232326005127\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, forward = hmm.decoder.run_forward(initial_scores, transition_scores,\n",
    "    final_scores, emission_scores)\n",
    "print 'Log-Likelihood =', log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood = -5.068232326005126\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, backward = hmm.decoder.run_backward(initial_scores, transition_scores,\n",
    "    final_scores, emission_scores)\n",
    "print 'Log-Likelihood =', log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.6 </b>\n",
    "<br>\n",
    "Compute the node posteriors for the first training sequence (use the provided compute posteriors func- tion), and look at the output. Note that the state posteriors are a proper probability distribution (the lines of the result sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95738152 0.04261848]\n",
      " [0.75281282 0.24718718]\n",
      " [0.26184794 0.73815206]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "state_posteriors, _, _ = hmm.compute_posteriors(initial_scores,transition_scores,final_scores,emission_scores)\n",
    " \n",
    "print state_posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.7 </b>\n",
    "<br>\n",
    "Run the posterior decode on the first test sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 0: walk/rainy walk/rainy shop/sunny clean/sunny \n",
      "Truth test 0:      walk/rainy walk/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0])\n",
    "print \"Prediction test 0:\", y_pred\n",
    "print \"Truth test 0:     \", simple.test.seq_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(y_seq, predicted_y_seq , number_of_classes):\n",
    "    accuracy=0\n",
    "    precision=0\n",
    "    recall=0\n",
    "    evaluations=0\n",
    "    for class_i in number_of_classes:\n",
    "        elements_correct= sum(1 for i in range(len(y_seq)) if predicted_y_seq[i]==class_i and predicted_y_seq[i]==y_seq[i])\n",
    "        if elements_correct != 0:\n",
    "            accuracy+=elements_correct #accuracy is the nº of elements correctly predicted of each class\n",
    "            precision+=elements_correct/(predicted_y_seq == class_i).sum(0) #precision of each class\n",
    "            recall+=elements_correct/ (y_seq).count(class_i)                #recall of class y\n",
    "            evaluations+=1\n",
    "    print \"\\nAccuracy:\", accuracy, \"\\n\"\n",
    "    if evaluations !=0:\n",
    "        print \"Precision:\", precision/evaluations, \"\\n\"\n",
    "        print \"Recall:\", recall/evaluations,\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 3 \n",
      "\n",
      "Precision: 0.75 \n",
      "\n",
      "Recall: 0.833333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(simple.test.seq_list[0].y, y_pred.y, simple.y_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do the same for the second test sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 1: clean/rainy walk/rainy tennis/rainy walk/rainy \n",
      "Truth test 1:      clean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "\n",
      "Accuracy: 0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lxmls/sequences/sequence_classifier.py:79: RuntimeWarning: invalid value encountered in subtract\n",
      "  state_posteriors[pos, :] -= log_likelihood\n",
      "lxmls/sequences/sequence_classifier.py:92: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  transition_posteriors[pos, state, prev_state] -= log_likelihood\n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "print \"Prediction test 1:\", y_pred\n",
    "print \"Truth test 1:     \", simple.test.seq_list[1]\n",
    "\n",
    "evaluate(simple.test.seq_list[1].y, y_pred.y, simple.y_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is wrong? Note the observations for the second test sequence: the observation tennis was never seen at training time, so the probability for it will be zero (no matter what state). This will make all possible state sequences have zero probability. As seen in the previous lecture, this is a problem with generative models, which can be corrected using smoothing (among other options).\n",
    "\n",
    "> <br> Change the train supervised method to add smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmm.train_supervised(simple.train, smoothing=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing Prediction test 0: walk/rainy walk/rainy shop/sunny clean/sunny \n",
      "Smoothing Truth test 0:      walk/rainy walk/sunny shop/sunny clean/sunny \n",
      "\n",
      "Accuracy: 3 \n",
      "\n",
      "Precision: 0.75 \n",
      "\n",
      "Recall: 0.833333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0])\n",
    "print \"Smoothing Prediction test 0:\", y_pred\n",
    "print \"Smoothing Truth test 0:     \", simple.test.seq_list[0]\n",
    "evaluate(simple.test.seq_list[0].y, y_pred.y, simple.y_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing Prediction test 1: clean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "Smoothing Truth test 1:      clean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "\n",
      "Accuracy: 4 \n",
      "\n",
      "Precision: 1.0 \n",
      "\n",
      "Recall: 1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "print \"Smoothing Prediction test 1:\", y_pred\n",
    "print \"Smoothing Truth test 1:     \", simple.test.seq_list[1]\n",
    "\n",
    "evaluate(simple.test.seq_list[1].y, y_pred.y, simple.y_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 2.8 - Viterbi </b>\n",
    "<br>\n",
    "Implement a method for performing Viterbi decoding in file sequence classification decoder.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#My implementation in the class decoder.py\n",
    "\n",
    "def run_viterbi(self, initial_scores, transition_scores, final_scores, emission_scores):\n",
    "\n",
    "    length = np.size(emission_scores, 0)  # Length of the sequence.\n",
    "    num_states = np.size(initial_scores)  # Number of states.\n",
    "\n",
    "    # Variables storing the Viterbi scores.\n",
    "    viterbi_scores = np.zeros([length, num_states]) + logzero()\n",
    "\n",
    "    # Variables storing the paths to backtrack.\n",
    "    viterbi_paths = -np.ones([length, num_states], dtype=int)\n",
    "\n",
    "    # Most likely sequence.\n",
    "    best_path = -np.ones(length, dtype=int)\n",
    "\n",
    "    # Initialization.\n",
    "    viterbi_scores[0, :] = emission_scores[0, :] + initial_scores\n",
    "\n",
    "    # Viterbi loop.\n",
    "    for pos in xrange(1, length):\n",
    "        for current_state in xrange(num_states):\n",
    "\n",
    "            viterbi_scores[pos, current_state]= np.max(viterbi_scores[pos-1, :] + transition_scores[pos-1, current_state, :])\n",
    "            viterbi_scores[pos, current_state]+=emission_scores[pos, current_state]\n",
    "            viterbi_paths[pos, current_state] = np.argmax(viterbi_scores[pos-1, :] + transition_scores[pos-1, current_state, :])\n",
    "\n",
    "    # Termination.\n",
    "    best_score = np.max(viterbi_scores[length-1, :] + final_scores)\n",
    "\n",
    "    #best state\n",
    "    best_path[-1]=np.argmax(viterbi_scores[length-1, :] + final_scores) \n",
    "\n",
    "    for i in xrange(length-2,-1,-1):\n",
    "        best_path[i]=viterbi_paths[i+1, best_path[i+1]]\n",
    "\n",
    "    return best_path, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding Prediction test 0 with smoothing: walk/rainy walk/rainy shop/sunny clean/sunny  -6.020501246982869 \n",
      "\n",
      "Truth test 0: walk/rainy walk/sunny shop/sunny clean/sunny  \n",
      "\n",
      "Viterbi decoding Prediction test 1 with smoothing: clean/sunny walk/sunny tennis/sunny walk/sunny  -11.713974073970887 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hmm.train_supervised(simple.train, smoothing=0.1)\n",
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[0])\n",
    "print \"Viterbi decoding Prediction test 0 with smoothing:\", y_pred, score, \"\\n\"\n",
    "\n",
    "print \"Truth test 0:\", simple.test.seq_list[0], \"\\n\"\n",
    "\n",
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[1])\n",
    "print \"Viterbi decoding Prediction test 1 with smoothing:\", y_pred, score, \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mypy26]",
   "language": "python",
   "name": "conda-env-mypy26-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
